{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement RAG using a website and watsonx.ai \n",
    "\n",
    "In the lab **\"Implement RAG Use Cases in watsonx.ai\"** we looked at how to implement a RAG use with our source being from some `.pdf` and `.txt` files. In this example we instead source of content by scraping a given website URL. \n",
    "\n",
    "The main difference from the previous example is how data is sourced for our embedding. We'll use open source APIs [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) and [spacy](https://spacy.io/) to get data from a Web page.\n",
    "\n",
    "To get started we'll first verify that you have the necessary dependencies installed to run this notebook.\n",
    "\n",
    "Go ahead and run the following code cell. **This may take a few seconds to complete.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in dependencies\n",
    "\n",
    "In this next code cell we'll bring in all the dependencies we'll need for later use.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 42.8/42.8 MB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.10.15)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\admin\\onedrive - bittek soluciones tecnológicas\\formacion\\generative_ai\\generative_ai\\watsonx_env\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in dependencies\n",
    "# SQLite fix: https://docs.trychroma.com/troubleshooting#sqlite\n",
    "# __import__('pysqlite3')\n",
    "# import sys\n",
    "# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import chromadb\n",
    "# import en_core_web_md\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important variables\n",
    "\n",
    "In this next code cell you'll define some variables that will be used in order to interact with your instance of watsonx.ai.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the global variables that will be used for authentication in another function\n",
    "watsonx_project_id = os.getenv('project_id', None)\n",
    "api_key = os.getenv('api_key', None)\n",
    "instance_url = \"https://us-south.ml.cloud.ibm.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the code\n",
    "\n",
    "In this next code cell we'll create some functions that we can use later to interact easier with watsonx.ai. These functions are `get_model`, `create_embedding`, and `create_prompt`: \n",
    "\n",
    "- `get_model`: Creates a model object that will be used to invoke the LLM\n",
    "- `extract_text`: Will pull text from a given website to create embedding from\n",
    "- `split_text_into_sentences`: Split the text we extracted into individual sentences and clean them of any unnecessary characters\n",
    "- `create_embedding`: Loads text data from a given URL into the in-memory `chromadb` instance\n",
    "- `create_prompt`: Generates the prompt that is sent to watsonx.ai API\n",
    "   - Notice that in the beginning of the function we query the vector database to retrieve information that’s related to our question (semantic search).\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_type, max_tokens, min_tokens, decoding, temperature, top_k, top_p):\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.TOP_K: top_k,\n",
    "        GenParams.TOP_P: top_p,\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": instance_url\n",
    "        },\n",
    "        project_id=watsonx_project_id\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract contents of <p> elements\n",
    "            p_contents = [p.get_text() for p in soup.find_all('p')]\n",
    "\n",
    "            # Print the contents of <p> elements\n",
    "            print(\"\\nContents of <p> elements: \\n\")\n",
    "            for content in p_contents:\n",
    "                print(content)\n",
    "            raw_web_text = \" \".join(p_contents)\n",
    "            # remove \\xa0 which is used in html to avoid words break acorss lines.\n",
    "            cleaned_text = raw_web_text.replace(\"\\xa0\", \" \")\n",
    "            return cleaned_text\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def split_text_into_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cleaned_sentences = [s.strip() for s in sentences]\n",
    "    return cleaned_sentences\n",
    "\n",
    "def create_embedding(url, collection_name):\n",
    "    cleaned_text = extract_text(url)\n",
    "    cleaned_sentences = split_text_into_sentences(cleaned_text)\n",
    "\n",
    "    client = chromadb.Client()\n",
    "\n",
    "    collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "    # Upload text to chroma\n",
    "    collection.upsert(\n",
    "        documents=cleaned_sentences,\n",
    "        metadatas=[{\"source\": str(i)} for i in range(len(cleaned_sentences))],\n",
    "        ids=[str(i) for i in range(len(cleaned_sentences))],\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "def create_prompt(url, question, collection_name):\n",
    "    # Create embeddings for the text file\n",
    "    collection = create_embedding(url, collection_name)\n",
    "\n",
    "    # query relevant information\n",
    "    relevant_chunks = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=5,\n",
    "    )\n",
    "    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n",
    "    # Please note that this is a generic format. You can change this format to be specific to llama\n",
    "    prompt = (f\"{context}\\n\\nPlease answer the following question in one sentence using this \"\n",
    "              + f\"text. \"\n",
    "              + f\"If the question is unanswerable, say \\\"unanswerable\\\". Do not include information that's not relevant to the question.\"\n",
    "              + f\"Question: {question}\")\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluing it together\n",
    "\n",
    "The next function, `answer_questions_from_web`, that we create is created to help combine the previous five that we defined. This is the wrapper that we will call when we want to interact with watsonx.ai. \n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions_from_web(url, question, collection_name):\n",
    "    # Specify model parameters\n",
    "    model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "    max_tokens = 100\n",
    "    min_tokens = 50\n",
    "    top_k = 50\n",
    "    top_p = 1\n",
    "    decoding = DecodingMethods.GREEDY\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Get the watsonx model = try both options\n",
    "    model = get_model(model_type, max_tokens, min_tokens, decoding, temperature, top_k, top_p)\n",
    "\n",
    "    # Get the prompt\n",
    "    complete_prompt = create_prompt(url, question, collection_name)\n",
    "\n",
    "    generated_response = model.generate(prompt=complete_prompt)\n",
    "    response_text = generated_response['results'][0]['generated_text']\n",
    "\n",
    "    # Remove trailing white spaces\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "    # print model response\n",
    "    print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "    print(response_text.strip())\n",
    "    print(\"*********************************************************************************************\")\n",
    "\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering some questions\n",
    "\n",
    "The next code cell will use all the previous code we've created so far to source information from the input documents and ask a question about them using watsonx.ai (Notice the return of the `answer_questions_from_web` function). \n",
    "\n",
    "To do so we'll pass in a question we want to ask, the web URL we want to reference for said question, and finally the name of the collection where the embeddings exist.\n",
    "\n",
    "Go ahead and run the next code cell. **You will see output from this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of <p> elements: \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "  \n",
      "  \n",
      "      Now available—a next generation enterprise studio for AI builders to train, validate, tune and deploy AI models\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "IBM® watsonx.ai™ AI studio is part of the IBM watsonx™ AI and data platform, bringing together new generative AI (gen AI) capabilities powered by foundation models and traditional machine learning (ML) into a powerful studio spanning the AI lifecycle. Tune and guide models with your enterprise data to meet your needs with easy-to-use tools for building and refining performant prompts. With watsonx.ai, you can build AI applications in a fraction of the time and with a fraction of the data. Watsonx.ai offers:\n",
      "\n",
      "IBM Offers Meta’s Llama 3 Open Models on Watsonx\n",
      "Chat directly with an LLM in the watsonx.ai 30-day demo. This demo does not include agents or simultaneous chat with multiple models.\n",
      "IDC Spotlight: The truth about successful generative AI\n",
      "Bring your own custom foundation models or work with a suite of curated foundation models offered by IBM. Experiment with open source models through the IBM and Hugging Face partnership to meet the needs of your business.\n",
      "Use open-source frameworks and tools for code-based, automated and visual data science capabilities–all in a secure, trusted studio environment.\n",
      "Leverage foundation models and generative AI with minimal data, advanced prompt-tuning capabilities, full SDK and API libraries.\n",
      "Accelerate the full AI model lifecycle with all the tools and runtimes in one place to train, validate, tune and deploy AI models across clouds and on-premises environments.\n",
      "In the Prompt Lab, leverage foundation models to create better AI, faster. Experiment with different prompts for various use cases and tasks. With just a few lines of instruction you can draft job descriptions, classify customer complaints, summarize complex regulatory documents, extract key business information and much more. Quickly tune models for your specific business needs using the latest open source and IBM trained foundation models.\n",
      "Build models either visually or with code, deploy and monitor with end-to-end lifecycle explainability and fairness. Use MLOps to simplify model production from any tool and provide automatic model retraining.\n",
      "Businesses are excited about the prospect of tapping foundation models and ML in one place, with their own data, to accelerate generative AI workloads.\n",
      "“Watsonx.ai proved to be very useful. In our research, we liked how it helped our customers (and our development team) to simplify tasks and extend the assistant knowledge without the need to pre-set the whole dialog in advance. It is a next level for us and our customers.”\n",
      "\n",
      "— Jindrich Chromy, CEO and co-founder\n",
      "“With this tool, we will need only a quarter of the time compared to before to plan, write and publish an article.”\n",
      "\n",
      "— Madeline Scholz\n",
      "“IBM’s commitment to cultivating an ecosystem of enterprise AI solutions based on their trustworthy and secure AI platform has proven highly complementary to our go-to-market strategy. We look forward to further cooperation.”\n",
      "\n",
      "— Tom Foley, Founder and CEO\n",
      "“We are just at the beginning of this journey. This endeavor with IBM has reinforced our conviction that legal intelligence is on the brink of a transformative era, and Blendow Group is poised to lead this revolution.”\n",
      "\n",
      "— Johan Wallquist, Chief Digital Innovation Officer\n",
      "“We are encouraged by the results of this pilot, and we look forward to the next chapter in our Generative AI journey. IBM’s ethical and transparent approach to AI helps us put human values at the center of our transformation initiative.” — Alceu Meinen, Superintendent of AI and Customer Relations\n",
      "“The IBM team has fully supported our patient-first approach that combines process research and process mining to establish areas of opportunity to improve the patient experience and outcomes. This has helped us to identify where we could pilot process changes and exciting new technologies to have an impact far more quickly than we would have before.\" — Professor Andy Hardy, Chief Executive Officer\n",
      "Take the next step to start operationalizing and scaling generative AI and ML for business.\n",
      "1IBM’s statements regarding its plans, directions, and intent are subject to change or withdrawal without notice at IBM’s sole discretion. See Pricing for more detail. Unless otherwise specified under Software pricing, all features, capabilities, and potential updates refer exclusively to SaaS. IBM makes no representation that SaaS and software features and capabilities will be the same.\n",
      "--------------------------------- Generated response -----------------------------------\n",
      "Prompt Lab is a tool that allows users to experiment with different prompts for various use cases and tasks and tune and guide models with their enterprise data to meet their needs with easy-to-use tools for building and refining performant prompts.\n",
      "*********************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Prompt Lab is a tool that allows users to experiment with different prompts for various use cases and tasks and tune and guide models with their enterprise data to meet their needs with easy-to-use tools for building and refining performant prompts.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try diffrent URLs and questions\n",
    "web_url = \"https://www.ibm.com/products/watsonx-ai\"\n",
    "question = \"What is Prompt Lab?\"\n",
    "collection_name = \"test_web_RAG\"\n",
    "\n",
    "answer_questions_from_web(web_url, question, collection_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
