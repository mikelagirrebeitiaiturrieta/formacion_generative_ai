{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario instalar el ejecutable ffmpeg para poder ejecutar este script. Seguir los pasos de instalación del siguiente repo sobre la <a href='https://github.com/jiaaro/pydub?tab=readme-ov-file#installation'>libreria pydub</a>.\n",
    "\n",
    "EN caso de recibir error al ejecutar la función Play(), sobre el fichero .py de play modificar la linea 14 de código sustituyendo por lo siguiente:\n",
    "```{python}\n",
    "with NamedTemporaryFile(\"w+b\", suffix=\".wav\", delete=False) as f:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instlación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install speechrecognition\n",
    "!pip install ibm_watson_machine_learning\n",
    "!pip install langchain\n",
    "!pip install load_dotenv\n",
    "!pip install ipython\n",
    "!pip install ipykernel\n",
    "!pip install gtts\n",
    "!pip install pydub\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\virtualenvs\\speech_2_speech\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "c:\\virtualenvs\\speech_2_speech\\Lib\\site-packages\\ibm_watson_machine_learning\\foundation_models\\extensions\\langchain\\llm.py:60: WatsonxLLMDeprecationWarning: ibm_watson_machine_learning.foundation_models.extensions.langchain.WatsonxLLM is deprecated and will not be supported in the future. Please import from langchain-ibm instead.\n",
      "To install langchain-ibm run `pip install -U langchain-ibm`.\n",
      "  _raise_watsonxllm_deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "try:\n",
    "    REGION = os.environ[\"RUNTIME_ENV_REGION\"]\n",
    "except KeyError:\n",
    "    # Set your region here if you are not running this notebook in the watsonx.ai Jupyter environment\n",
    "    # us-south, eu-de, etc.\n",
    "    REGION = \"us-south\"\n",
    "\n",
    "try:\n",
    "    api_key = os.environ[\"api_key\"]\n",
    "except KeyError:\n",
    "    # Enter api key here if not running this notebook in the watsonx.ai Jupyter environment\n",
    "    api_key = getpass.getpass(\"Please enter your WML api key (hit enter): \")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://\" + REGION + \".ml.cloud.ibm.com\",\n",
    "    \"apikey\": api_key\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    project_id = os.environ[\"project_id\"]\n",
    "except KeyError:\n",
    "    # Enter project ID here if not running this notebook in the watsonx.ai Jupyter environment\n",
    "    project_id = getpass.getpass(\"Please enter your WML project_id (hit enter): \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesamos el audio y con el modelo de reconocimiento de voz de google transformamos el audio a texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_to_wav(ogg_file, wav_file, format='ogg'):\n",
    "    audio = AudioSegment.from_file(ogg_file, format=format)\n",
    "    audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "def audio_to_text(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data, language='es-ES')\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            return \"No se pudo entender el audio\"\n",
    "        except sr.RequestError as e:\n",
    "            return f\"Error de solicitud; {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el LLM para generar una respuesta al audio, y lo transformamos a formato audio con la libreria de transformación de texto a speech de google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\virtualenvs\\speech_2_speech\\Lib\\site-packages\\ibm_watson_machine_learning\\foundation_models\\utils\\utils.py:290: LifecycleWarning: Model 'ibm-mistralai/mixtral-8x7b-instruct-v01-q' is in deprecated and constricted state from 2024-04-19 until 2024-06-20. IDs of alternative models: ibm-mistralai/mixtral-8x7b-instruct-v01. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(default_warning_template.format(\n"
     ]
    }
   ],
   "source": [
    "def get_model(model_id, credentials, project_id):\n",
    "    model_id_1 = model_id\n",
    "    parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE,\n",
    "    GenParams.MAX_NEW_TOKENS: 50,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.STOP_SEQUENCES: [\".\"]\n",
    "}\n",
    "    model = Model(\n",
    "        model_id=model_id_1,  \n",
    "        params=parameters, \n",
    "        credentials=credentials,\n",
    "        project_id=project_id)\n",
    "    \n",
    "    llm = WatsonxLLM(model=model)\n",
    "    return llm\n",
    "\n",
    "def generate_response(model, prompt):\n",
    "    response = model.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "def text_to_audio(text, output_file):\n",
    "    tts = gTTS(text=text, lang='es')\n",
    "    tts.save(output_file)\n",
    "\n",
    "\n",
    "llm = get_model(ModelTypes.MIXTRAL_8X7B_INSTRUCT_V01_Q, credentials, project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinamos todas las funciones previas y mostramos el audio generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(origin_file, wav_file, llm, response_audio_file):\n",
    "    convert_audio_to_wav(origin_file, wav_file, format='ogg')\n",
    "    text = audio_to_text(wav_file)\n",
    "    print(f\"Texto reconocido: {text}\")\n",
    "    \n",
    "    response = generate_response(llm, text)\n",
    "    print(f\"Respuesta generada: {response}\")\n",
    "    \n",
    "    text_to_audio(response, response_audio_file)\n",
    "    response_audio = AudioSegment.from_mp3(response_audio_file)\n",
    "    \n",
    "    play(response_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto reconocido: qué es la Inteligencia Artificial generativa\n",
      "Respuesta generada: \n",
      "\n",
      "La Inteligencia Artificial generativa (IA generativa) es una subcategoría de la Inteligencia Artificial (IA) que se centra en la creación de contenido nuevo y original.\n"
     ]
    }
   ],
   "source": [
    "# Nombre del archivo OGG de entrada y el archivo MP3 de salida\n",
    "origin_file = \"data/audio.ogg\"\n",
    "wav_file = \"data/audio.wav\"\n",
    "response_audio_file = \"data/response.mp3\"\n",
    "\n",
    "# Procesar el audio\n",
    "process_audio(origin_file=origin_file, wav_file=wav_file, llm=llm, response_audio_file=response_audio_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_2_speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
