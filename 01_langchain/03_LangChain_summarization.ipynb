{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summarization with langchain\n",
    "\n",
    "Summarization of long documents is a common LLM use case. The issue that most often arises, however, is that there is a token limit for the model. (Max context window length). With langchain this can be worked around by chunking and recursive summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# First import the dependencies we need:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Get our API key, projectId and URL from .env\n",
    "load_dotenv('../../.env')\n",
    "api_key = os.getenv(\"api_key\", None)\n",
    "ibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None)\n",
    "project_id = os.getenv(\"project_id\", None)\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can take a [stuff](https://python.langchain.com/docs/modules/chains/document/stuff) or [map reduce](https://python.langchain.com/docs/modules/chains/document/map_reduce) approach to summarizing documents. We'll start with the simpler \"stuff\". Feel free to play around with changing the document URL and inference parameters to optimize the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading web document...\n",
      "Done.\n",
      "Initializing flan-ul2-20B model...\n",
      "Running summarization task...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\OneDrive - Bittek Soluciones Tecnológicas\\FORMACION\\generative_ai\\generative_ai\\watsonx_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI) and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data. These insights can be used to guide decision making and strategic planning.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize llm and document loader:\n",
    "print(\"Loading web document...\")\n",
    "# Try out some other documents as well\n",
    "loader = WebBaseLoader(\"https://www.ibm.com/topics/data-science\")\n",
    "doc = loader.load()\n",
    "print(\"Done.\")\n",
    "\n",
    "# You might need to tweak some of the runtime parameters to optimize the results.\n",
    "print(\"Initializing flan-ul2-20B model...\")\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.15,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 20,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 20,\n",
    "    GenParams.MAX_NEW_TOKENS: 205\n",
    "}\n",
    "\n",
    "flan_model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "# Can use 'stuff' or 'map reduce'; \n",
    "chain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n",
    "\n",
    "print(\"Running summarization task...\\n\")\n",
    "\n",
    "res = chain.run(doc)\n",
    "\n",
    "print(res)\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine several of the features we've seen previously, including prompt templates and chains. In the following block we load the document into a template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing chain...\n",
      "Stuff chain with documents...\n",
      "Running summarization on stuffed document chain...\n",
      "\n",
      "Data science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI) and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data. These insights can be used to guide decision making and strategic planning.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "print(\"Initializing chain...\")\n",
    "llm_chain = LLMChain(llm=flan_model, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "print(\"Stuff chain with documents...\")\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "print(\"Running summarization on stuffed document chain...\\n\")\n",
    "res = stuff_chain.run(doc)\n",
    "\n",
    "print(res)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output above should be the same as the previous block if using the same inference parameters and document URL. Now we will use the same stuff chain method to see how it behaves with multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2nd article...\n",
      "Done.\n",
      "Running summarization on stuffed document chain.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-25)\n",
      "Status code: 400, body: {\"errors\":[{\"code\":\"invalid_input_argument\",\"message\":\"Invalid input argument for Model 'google/flan-ul2': the number of input tokens 10496 cannot exceed the total tokens limit 4096 for this model\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai\"}],\"trace\":\"dc0ec94becfde6927a05a48487b0dd63\",\"status_code\":400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-25)\n",
      "Status code: 400, body: {\"errors\":[{\"code\":\"invalid_input_argument\",\"message\":\"Invalid input argument for Model 'google/flan-ul2': the number of input tokens 10496 cannot exceed the total tokens limit 4096 for this model\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai\"}],\"trace\":\"dc0ec94becfde6927a05a48487b0dd63\",\"status_code\":400}\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Add a new article\n",
    "print(\"Loading 2nd article...\")\n",
    "loader_2 = WebBaseLoader('https://emeritus.org/in/learn/what-are-the-roles-and-responsibilities-of-a-data-scientist/')\n",
    "doc_2 = loader_2.load() # Returns list\n",
    "print(\"Done.\")\n",
    "\n",
    "# Combine docs\n",
    "docs = doc + doc_2\n",
    "\n",
    "print(\"Running summarization on stuffed document chain.\\n\")\n",
    "try:\n",
    "  res = stuff_chain.run(docs)\n",
    "  print(res)\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the above code should result in an error to the effect of `input tokens (4811) plus prefix length (0) must be < 4096` meaning that we have exceeded the model's token input length. This brings us to the next topic, \"map reduce\" which helps us solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\OneDrive - Bittek Soluciones Tecnológicas\\FORMACION\\generative_ai\\generative_ai\\watsonx_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3rd document...\n",
      "Init map chain...\n",
      "Init reduce chain...\n",
      "Stuff documents using reduce chain...\n",
      "Init chunk splitter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3515 > 2048). Running this sequence through the model will result in indexing errors\n",
      "c:\\Users\\admin\\OneDrive - Bittek Soluciones Tecnológicas\\FORMACION\\generative_ai\\generative_ai\\watsonx_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 chunks: \n",
      "Run map-reduce chain. This should take ~15-30 seconds...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Elapsed time: 27.54 seconds.\n",
      "\n",
      "Results from each chunk: \n",
      "\n",
      "1. Data science combines math and statistics, specialized programming, advanced analytics, artificial intelligence (AI) and machine learning with specific subject matter expertise to uncover actionable insights hidden in an organization’s data. These insights can be used to guide decision making and strategic planning.\n",
      "\n",
      "2. Businesses in many different areas use data science to guide their decisions. It is the responsibility of a data scientist to take huge amounts of complex information and analyze it in such a way that the decision-makers within a business, organization or other body can understand it. The role of a data scientist may be more crucial than ever in today’s data-driven environment. All the information you require to become a data scientist is provided here. In this article, let’s learn more about the roles and responsibilities of a data scientist. Who is a Data Scientist? Data scientists are analytical data experts who possess the technical skills to address complex problems. They gather, analyze, and interpret vast amounts of data while working with a variety of computer science, mathematics, and statistics-related concepts. They have a duty to offer perspectives that go beyond statistical analysis. Data scientist positions are accessible in both the public and private sectors, including finance, consulting, manufacturing, pharmaceuticals, government\n",
      "\n",
      "3. Data Science Career Career Accelerator Roles and Responsibilities of a Data Scientist\n",
      "\n",
      "4. data-science - emeritus-online-programs-data-science\n",
      "\n",
      "5. Data science is a booming industry. Try your hand at these projects to develop your skills and keep up with the latest trends.Written byClaire D. CostaClaire D. CostaContent Crafter at DigitalogyClaire D. Costa is a tech-focused content crafter at Digitalogy, a company that helps businesses connect with top software engineering talent. Image: Shutterstock / Built In Hal KossSenior Associate Editor at Built InUPDATED BYHal Koss | Mar 12, 2024 Data science is a profession that requires a variety of scientific tools, processes, algorithms and knowledge extraction systems that are used to identify meaningful patterns in structured and unstructured data alike.If you fancy data science and are eager to get a solid grip on the technology, now is as good a time as ever to hone your skills to comprehend and manage the upcoming challenges facing the profession.\n",
      "\n",
      "\n",
      "\n",
      "Final output:\n",
      "\n",
      "Data science is a rapidly growing field that combines statistics, computer science, and business to help organizations make informed decisions.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "# Add a 3rd document\n",
    "print(\"Loading 3rd document...\")\n",
    "loader_3 = WebBaseLoader(\"https://builtin.com/data-science/data-science-projects\")\n",
    "doc_3 = loader_3.load()\n",
    "docs = docs + doc_3\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "print(\"Init map chain...\")\n",
    "map_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "print(\"Init reduce chain...\")\n",
    "reduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "print(\"Stuff documents using reduce chain...\")\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n",
    "# You might want to play around with different tokenizers and text splitters to see how the results change.\n",
    "print(\"Init chunk splitter...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\") # Hugging face tokenizer for flan-ul2\n",
    "    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    print(f\"Using {len(split_docs)} chunks: \")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print(\"Run map-reduce chain. This should take ~15-30 seconds...\")\n",
    "try:\n",
    "    t1_start = perf_counter()\n",
    "    results = map_reduce_chain(split_docs)\n",
    "    steps = results[\"intermediate_steps\"]\n",
    "    output = results[\"output_text\"]\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n",
    "\n",
    "    print(\"Results from each chunk: \\n\")\n",
    "    for idx, step in enumerate(steps):\n",
    "        print(f\"{idx + 1}. {step}\\n\")\n",
    "    \n",
    "    print(\"\\n\\nFinal output:\\n\")\n",
    "    print(output)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your own Summarization workflow\n",
    "Taking the previous code lines as example, try to create your own summarization workflow. It would be interesting to use different models and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
